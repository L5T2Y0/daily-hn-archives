# Weekly Summary æ¨¡å— - è´Ÿè´£ç”Ÿæˆå‘¨æŠ¥
from datetime import datetime, timedelta, timezone
from pathlib import Path
import re
from tag_classifier import add_tags_to_article, group_articles_by_tag, get_tag_statistics, format_tags_for_display


def get_week_date_range() -> tuple[str, str]:
    """
    è·å–ä¸Šå‘¨çš„æ—¥æœŸèŒƒå›´ï¼ˆå‘¨ä¸€åˆ°å‘¨æ—¥ï¼‰
    
    è¿”å›:
        (start_date, end_date) æ ¼å¼ä¸º YYYY-MM-DD
    """
    beijing_tz = timezone(timedelta(hours=8))
    today = datetime.now(beijing_tz).date()
    
    # è®¡ç®—ä¸Šå‘¨ä¸€å’Œä¸Šå‘¨æ—¥
    weekday = today.weekday()  # 0=å‘¨ä¸€, 6=å‘¨æ—¥
    last_sunday = today - timedelta(days=weekday + 1)  # ä¸Šå‘¨æ—¥
    last_monday = last_sunday - timedelta(days=6)  # ä¸Šå‘¨ä¸€
    
    return last_monday.strftime("%Y-%m-%d"), last_sunday.strftime("%Y-%m-%d")


def parse_archive_file(file_path: Path) -> list[dict]:
    """
    è§£æå½’æ¡£æ–‡ä»¶ï¼Œæå–æ–‡ç« ä¿¡æ¯
    
    å‚æ•°:
        file_path: å½’æ¡£æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯ç¯‡æ–‡ç« åŒ…å« title, url, score, comments
    """
    try:
        content = file_path.read_text(encoding="utf-8")
        articles = []
        
        # åŒ¹é…æ ¼å¼: 1. [title](url) - score points, comments comments
        pattern = r'\d+\.\s+\[(.+?)\]\((.+?)\)\s+-\s+(\d+)\s+points,\s+(\d+)\s+comments'
        matches = re.findall(pattern, content)
        
        for title, url, score, comments in matches:
            articles.append({
                "title": title,
                "url": url,
                "score": int(score),
                "comments": int(comments)
            })
        
        return articles
    except Exception:
        return []


def collect_week_articles(start_date: str, end_date: str) -> list[dict]:
    """
    æ”¶é›†ä¸€å‘¨å†…çš„æ‰€æœ‰æ–‡ç« 
    
    å‚æ•°:
        start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD
    
    è¿”å›:
        æ‰€æœ‰æ–‡ç« åˆ—è¡¨
    """
    archives_dir = Path("archives")
    all_articles = []
    
    start = datetime.strptime(start_date, "%Y-%m-%d").date()
    end = datetime.strptime(end_date, "%Y-%m-%d").date()
    
    current = start
    while current <= end:
        file_path = archives_dir / f"{current.strftime('%Y-%m-%d')}.md"
        if file_path.exists():
            articles = parse_archive_file(file_path)
            all_articles.extend(articles)
        current += timedelta(days=1)
    
    return all_articles


def rank_articles(articles: list[dict], top_n: int = 20) -> list[dict]:
    """
    å¯¹æ–‡ç« è¿›è¡Œæ’åï¼ˆæŒ‰åˆ†æ•°é™åºï¼‰
    
    å‚æ•°:
        articles: æ–‡ç« åˆ—è¡¨
        top_n: è¿”å›å‰Nç¯‡
    
    è¿”å›:
        æ’ååçš„æ–‡ç« åˆ—è¡¨
    """
    # å»é‡ï¼ˆç›¸åŒURLåªä¿ç•™åˆ†æ•°æœ€é«˜çš„ï¼‰
    unique_articles = {}
    for article in articles:
        url = article["url"]
        if url not in unique_articles or article["score"] > unique_articles[url]["score"]:
            unique_articles[url] = article
    
    # æŒ‰åˆ†æ•°æ’åº
    sorted_articles = sorted(unique_articles.values(), key=lambda x: x["score"], reverse=True)
    
    return sorted_articles[:top_n]


def generate_weekly_content(start_date: str, end_date: str, top_articles: list[dict]) -> str:
    """
    ç”Ÿæˆå‘¨æŠ¥å†…å®¹
    
    å‚æ•°:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        top_articles: çƒ­é—¨æ–‡ç« åˆ—è¡¨
    
    è¿”å›:
        Markdown æ ¼å¼çš„å‘¨æŠ¥å†…å®¹
    """
    lines = []
    
    # ä¸ºæ–‡ç« æ·»åŠ æ ‡ç­¾
    for article in top_articles:
        add_tags_to_article(article)
    
    # æ ‡é¢˜
    lines.append(f"# ğŸ“Š Hacker News å‘¨æŠ¥")
    lines.append(f"## {start_date} è‡³ {end_date}")
    lines.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    lines.append("### ğŸ“ˆ æœ¬å‘¨ç»Ÿè®¡")
    lines.append("")
    total_score = sum(article["score"] for article in top_articles)
    total_comments = sum(article["comments"] for article in top_articles)
    lines.append(f"- ğŸ“ æ”¶å½•æ–‡ç« ï¼š{len(top_articles)} ç¯‡")
    lines.append(f"- â­ æ€»ç‚¹èµæ•°ï¼š{total_score:,}")
    lines.append(f"- ğŸ’¬ æ€»è¯„è®ºæ•°ï¼š{total_comments:,}")
    lines.append("")
    
    # æ ‡ç­¾ç»Ÿè®¡
    tag_stats = get_tag_statistics(top_articles)
    lines.append("### ğŸ·ï¸ çƒ­é—¨æ ‡ç­¾")
    lines.append("")
    for tag, count in list(tag_stats.items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæ ‡ç­¾
        lines.append(f"- {format_tags_for_display([tag])}: {count} ç¯‡")
    lines.append("")
    
    # Top 20 æ–‡ç« 
    lines.append("### ğŸ”¥ æœ¬å‘¨ Top 20 çƒ­é—¨æ–‡ç« ")
    lines.append("")
    
    for i, article in enumerate(top_articles, 1):
        title = article["title"]
        url = article["url"]
        score = article["score"]
        comments = article["comments"]
        tags = format_tags_for_display(article.get("tags", []))
        
        lines.append(f"{i}. [{title}]({url})")
        lines.append(f"   - â­ {score} points | ğŸ’¬ {comments} comments")
        if tags:
            lines.append(f"   - {tags}")
        lines.append("")
    
    # é¡µè„š
    lines.append("---")
    beijing_tz = timezone(timedelta(hours=8))
    beijing_time = datetime.now(beijing_tz)
    timestamp = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
    lines.append(f"*å‘¨æŠ¥ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)*")
    lines.append("*æ•°æ®æ¥æº: [Hacker News API](https://github.com/HackerNews/API)*")
    
    return "\n".join(lines)


def update_readme_weekly_section(weekly_summary: str) -> None:
    """
    æ›´æ–° README ä¸­çš„å‘¨æŠ¥åŒºåŸŸ
    
    å‚æ•°:
        weekly_summary: å‘¨æŠ¥æ‘˜è¦å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
    """
    readme_path = Path("README.md")
    
    if not readme_path.exists():
        return
    
    content = readme_path.read_text(encoding="utf-8")
    
    # å¦‚æœæ²¡æœ‰å‘¨æŠ¥åŒºåŸŸæ ‡è®°ï¼Œåœ¨æ—¥æŠ¥åŒºåŸŸåæ·»åŠ 
    if "<!-- WEEKLY_SUMMARY_START -->" not in content:
        # åœ¨æ—¥æŠ¥åŒºåŸŸåæ·»åŠ å‘¨æŠ¥åŒºåŸŸ
        daily_end = content.find("<!-- DAILY_ARTICLES_END -->")
        if daily_end != -1:
            insert_pos = content.find("\n---\n", daily_end)
            if insert_pos != -1:
                weekly_section = f"\n\n## ğŸ“Š æœ¬å‘¨çƒ­é—¨\n\n<!-- WEEKLY_SUMMARY_START -->\n{weekly_summary}\n<!-- WEEKLY_SUMMARY_END -->\n"
                content = content[:insert_pos] + weekly_section + content[insert_pos:]
    else:
        # æ›´æ–°ç°æœ‰å‘¨æŠ¥åŒºåŸŸ
        pattern = r'<!-- WEEKLY_SUMMARY_START -->.*?<!-- WEEKLY_SUMMARY_END -->'
        replacement = f'<!-- WEEKLY_SUMMARY_START -->\n{weekly_summary}\n<!-- WEEKLY_SUMMARY_END -->'
        content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    readme_path.write_text(content, encoding="utf-8")


def generate_weekly_summary() -> None:
    """
    ç”Ÿæˆå‘¨æŠ¥çš„ä¸»å‡½æ•°
    """
    print("=" * 50)
    print("Weekly Summary - å¼€å§‹ç”Ÿæˆå‘¨æŠ¥")
    print("=" * 50)
    print()
    
    # è·å–æœ¬å‘¨æ—¥æœŸèŒƒå›´
    start_date, end_date = get_week_date_range()
    print(f"å‘¨æœŸ: {start_date} è‡³ {end_date}")
    print()
    
    # æ”¶é›†æœ¬å‘¨æ–‡ç« 
    print("æ­¥éª¤ 1: æ”¶é›†æœ¬å‘¨æ–‡ç« ")
    all_articles = collect_week_articles(start_date, end_date)
    print(f"å…±æ”¶é›† {len(all_articles)} ç¯‡æ–‡ç« ")
    print()
    
    if not all_articles:
        print("âš ï¸  æœ¬å‘¨æš‚æ— æ–‡ç« æ•°æ®")
        return
    
    # æ’åæ–‡ç« 
    print("æ­¥éª¤ 2: æ–‡ç« æ’å")
    top_articles = rank_articles(all_articles, top_n=20)
    print(f"Top 20 æ–‡ç« å·²é€‰å‡º")
    print()
    
    # ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶
    print("æ­¥éª¤ 3: ç”Ÿæˆå‘¨æŠ¥æ–‡ä»¶")
    weekly_content = generate_weekly_content(start_date, end_date, top_articles)
    
    # ä¿å­˜å‘¨æŠ¥æ–‡ä»¶
    weekly_dir = Path("weekly")
    weekly_dir.mkdir(exist_ok=True)
    
    # ä½¿ç”¨å‘¨æ—¥æ—¥æœŸä½œä¸ºæ–‡ä»¶å
    weekly_file = weekly_dir / f"week-{end_date}.md"
    weekly_file.write_text(weekly_content, encoding="utf-8")
    print(f"å‘¨æŠ¥æ–‡ä»¶å·²ä¿å­˜: {weekly_file}")
    print()
    
    # ç”ŸæˆREADMEæ‘˜è¦ï¼ˆTop 10ï¼‰
    print("æ­¥éª¤ 4: æ›´æ–° README")
    beijing_tz = timezone(timedelta(hours=8))
    beijing_time = datetime.now(beijing_tz)
    timestamp = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
    
    summary_lines = [f"> ğŸ• æœ€åæ›´æ–°ï¼š{timestamp} (åŒ—äº¬æ—¶é—´)", ""]
    summary_lines.append(f"**æœ¬å‘¨çƒ­é—¨ ({start_date} è‡³ {end_date})**")
    summary_lines.append("")
    
    for i, article in enumerate(top_articles[:10], 1):
        tags = format_tags_for_display(article.get("tags", []))
        summary_lines.append(f"{i}. [{article['title']}]({article['url']}) - {article['score']} points, {article['comments']} comments")
        if tags:
            summary_lines.append(f"   - {tags}")
    
    summary_lines.append("")
    summary_lines.append(f"ğŸ“ **[æŸ¥çœ‹å®Œæ•´å‘¨æŠ¥](weekly/week-{end_date}.md)** | Top 20 çƒ­é—¨æ–‡ç« ")
    
    update_readme_weekly_section("\n".join(summary_lines))
    print("README å·²æ›´æ–°")
    print()
    
    print("=" * 50)
    print("âœ“ å‘¨æŠ¥ç”Ÿæˆå®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    generate_weekly_summary()
