# Monthly Summary æ¨¡å— - è´Ÿè´£ç”ŸæˆæœˆæŠ¥
from datetime import datetime, timedelta, timezone
from pathlib import Path
import re
from tag_classifier import add_tags_to_article, group_articles_by_tag, get_tag_statistics, format_tags_for_display


def get_last_month_range() -> tuple[str, str]:
    """
    è·å–ä¸Šä¸ªæœˆçš„æ—¥æœŸèŒƒå›´
    
    è¿”å›:
        (start_date, end_date) æ ¼å¼ä¸º YYYY-MM-DD
    """
    beijing_tz = timezone(timedelta(hours=8))
    today = datetime.now(beijing_tz).date()
    
    # è®¡ç®—ä¸Šä¸ªæœˆçš„ç¬¬ä¸€å¤©
    first_day_this_month = today.replace(day=1)
    last_day_last_month = first_day_this_month - timedelta(days=1)
    first_day_last_month = last_day_last_month.replace(day=1)
    
    return first_day_last_month.strftime("%Y-%m-%d"), last_day_last_month.strftime("%Y-%m-%d")


def parse_archive_file(file_path: Path) -> list[dict]:
    """
    è§£æå½’æ¡£æ–‡ä»¶ï¼Œæå–æ–‡ç« ä¿¡æ¯
    
    å‚æ•°:
        file_path: å½’æ¡£æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯ç¯‡æ–‡ç« åŒ…å« title, url, score, comments
    """
    try:
        content = file_path.read_text(encoding="utf-8")
        articles = []
        
        # åŒ¹é…æ ¼å¼: 1. [title](url) - score points, comments comments
        pattern = r'\d+\.\s+\[(.+?)\]\((.+?)\)\s+-\s+(\d+)\s+points,\s+(\d+)\s+comments'
        matches = re.findall(pattern, content)
        
        for title, url, score, comments in matches:
            articles.append({
                "title": title,
                "url": url,
                "score": int(score),
                "comments": int(comments)
            })
        
        return articles
    except Exception:
        return []


def collect_month_articles(start_date: str, end_date: str) -> list[dict]:
    """
    æ”¶é›†ä¸€ä¸ªæœˆå†…çš„æ‰€æœ‰æ–‡ç« 
    
    å‚æ•°:
        start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD
    
    è¿”å›:
        æ‰€æœ‰æ–‡ç« åˆ—è¡¨
    """
    archives_dir = Path("archives")
    all_articles = []
    
    start = datetime.strptime(start_date, "%Y-%m-%d").date()
    end = datetime.strptime(end_date, "%Y-%m-%d").date()
    
    current = start
    while current <= end:
        file_path = archives_dir / f"{current.strftime('%Y-%m-%d')}.md"
        if file_path.exists():
            articles = parse_archive_file(file_path)
            all_articles.extend(articles)
        current += timedelta(days=1)
    
    return all_articles


def rank_articles(articles: list[dict], top_n: int = 50) -> list[dict]:
    """
    å¯¹æ–‡ç« è¿›è¡Œæ’åï¼ˆæŒ‰åˆ†æ•°é™åºï¼‰
    
    å‚æ•°:
        articles: æ–‡ç« åˆ—è¡¨
        top_n: è¿”å›å‰Nç¯‡
    
    è¿”å›:
        æ’ååçš„æ–‡ç« åˆ—è¡¨
    """
    # å»é‡ï¼ˆç›¸åŒURLåªä¿ç•™åˆ†æ•°æœ€é«˜çš„ï¼‰
    unique_articles = {}
    for article in articles:
        url = article["url"]
        if url not in unique_articles or article["score"] > unique_articles[url]["score"]:
            unique_articles[url] = article
    
    # æŒ‰åˆ†æ•°æ’åº
    sorted_articles = sorted(unique_articles.values(), key=lambda x: x["score"], reverse=True)
    
    return sorted_articles[:top_n]


def generate_monthly_content(start_date: str, end_date: str, top_articles: list[dict]) -> str:
    """
    ç”ŸæˆæœˆæŠ¥å†…å®¹
    
    å‚æ•°:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        top_articles: çƒ­é—¨æ–‡ç« åˆ—è¡¨
    
    è¿”å›:
        Markdown æ ¼å¼çš„æœˆæŠ¥å†…å®¹
    """
    lines = []
    
    # ä¸ºæ–‡ç« æ·»åŠ æ ‡ç­¾
    for article in top_articles:
        add_tags_to_article(article)
    
    # æå–å¹´æœˆ
    year_month = datetime.strptime(start_date, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ")
    
    # æ ‡é¢˜
    lines.append(f"# ğŸ“Š Hacker News æœˆæŠ¥")
    lines.append(f"## {year_month}")
    lines.append(f"### {start_date} è‡³ {end_date}")
    lines.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    lines.append("### ğŸ“ˆ æœ¬æœˆç»Ÿè®¡")
    lines.append("")
    total_score = sum(article["score"] for article in top_articles)
    total_comments = sum(article["comments"] for article in top_articles)
    avg_score = total_score // len(top_articles) if top_articles else 0
    lines.append(f"- ğŸ“ æ”¶å½•æ–‡ç« ï¼š{len(top_articles)} ç¯‡")
    lines.append(f"- â­ æ€»ç‚¹èµæ•°ï¼š{total_score:,}")
    lines.append(f"- ğŸ’¬ æ€»è¯„è®ºæ•°ï¼š{total_comments:,}")
    lines.append(f"- ğŸ“Š å¹³å‡ç‚¹èµï¼š{avg_score} points")
    lines.append("")
    
    # æ ‡ç­¾ç»Ÿè®¡
    tag_stats = get_tag_statistics(top_articles)
    lines.append("### ğŸ·ï¸ çƒ­é—¨æ ‡ç­¾")
    lines.append("")
    for tag, count in list(tag_stats.items())[:8]:  # æ˜¾ç¤ºå‰8ä¸ªæ ‡ç­¾
        lines.append(f"- {format_tags_for_display([tag])}: {count} ç¯‡")
    lines.append("")
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„
    grouped = group_articles_by_tag(top_articles)
    lines.append("### ğŸ“‚ åˆ†ç±»æµè§ˆ")
    lines.append("")
    for tag in list(tag_stats.keys())[:5]:  # æ˜¾ç¤ºå‰5ä¸ªåˆ†ç±»
        if tag in grouped:
            lines.append(f"#### {format_tags_for_display([tag])} ({len(grouped[tag])} ç¯‡)")
            lines.append("")
            for article in grouped[tag][:5]:  # æ¯ä¸ªåˆ†ç±»æ˜¾ç¤ºå‰5ç¯‡
                lines.append(f"- [{article['title']}]({article['url']}) - {article['score']} points")
            lines.append("")
    
    # Top 50 æ–‡ç« 
    lines.append("### ğŸ”¥ æœ¬æœˆ Top 50 çƒ­é—¨æ–‡ç« ")
    lines.append("")
    
    for i, article in enumerate(top_articles, 1):
        title = article["title"]
        url = article["url"]
        score = article["score"]
        comments = article["comments"]
        tags = format_tags_for_display(article.get("tags", []))
        
        lines.append(f"{i}. [{title}]({url})")
        lines.append(f"   - â­ {score} points | ğŸ’¬ {comments} comments")
        if tags:
            lines.append(f"   - {tags}")
        lines.append("")
    
    # é¡µè„š
    lines.append("---")
    beijing_tz = timezone(timedelta(hours=8))
    beijing_time = datetime.now(beijing_tz)
    timestamp = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
    lines.append(f"*æœˆæŠ¥ç”Ÿæˆæ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)*")
    lines.append("*æ•°æ®æ¥æº: [Hacker News API](https://github.com/HackerNews/API)*")
    
    return "\n".join(lines)


def update_readme_monthly_section(monthly_summary: str) -> None:
    """
    æ›´æ–° README ä¸­çš„æœˆæŠ¥åŒºåŸŸ
    
    å‚æ•°:
        monthly_summary: æœˆæŠ¥æ‘˜è¦å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
    """
    readme_path = Path("README.md")
    
    if not readme_path.exists():
        return
    
    content = readme_path.read_text(encoding="utf-8")
    
    # å¦‚æœæ²¡æœ‰æœˆæŠ¥åŒºåŸŸæ ‡è®°ï¼Œåœ¨å‘¨æŠ¥åŒºåŸŸåæ·»åŠ 
    if "<!-- MONTHLY_SUMMARY_START -->" not in content:
        # åœ¨å‘¨æŠ¥åŒºåŸŸåæ·»åŠ æœˆæŠ¥åŒºåŸŸ
        weekly_end = content.find("<!-- WEEKLY_SUMMARY_END -->")
        if weekly_end != -1:
            insert_pos = content.find("\n---\n", weekly_end)
            if insert_pos != -1:
                monthly_section = f"\n\n## ğŸ“… æœ¬æœˆç²¾é€‰\n\n<!-- MONTHLY_SUMMARY_START -->\n{monthly_summary}\n<!-- MONTHLY_SUMMARY_END -->\n"
                content = content[:insert_pos] + monthly_section + content[insert_pos:]
    else:
        # æ›´æ–°ç°æœ‰æœˆæŠ¥åŒºåŸŸ
        pattern = r'<!-- MONTHLY_SUMMARY_START -->.*?<!-- MONTHLY_SUMMARY_END -->'
        replacement = f'<!-- MONTHLY_SUMMARY_START -->\n{monthly_summary}\n<!-- MONTHLY_SUMMARY_END -->'
        content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    readme_path.write_text(content, encoding="utf-8")


def generate_monthly_summary() -> None:
    """
    ç”ŸæˆæœˆæŠ¥çš„ä¸»å‡½æ•°
    """
    print("=" * 50)
    print("Monthly Summary - å¼€å§‹ç”ŸæˆæœˆæŠ¥")
    print("=" * 50)
    print()
    
    # è·å–ä¸Šä¸ªæœˆæ—¥æœŸèŒƒå›´
    start_date, end_date = get_last_month_range()
    year_month = datetime.strptime(start_date, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ")
    print(f"å‘¨æœŸ: {year_month} ({start_date} è‡³ {end_date})")
    print()
    
    # æ”¶é›†æœ¬æœˆæ–‡ç« 
    print("æ­¥éª¤ 1: æ”¶é›†æœ¬æœˆæ–‡ç« ")
    all_articles = collect_month_articles(start_date, end_date)
    print(f"å…±æ”¶é›† {len(all_articles)} ç¯‡æ–‡ç« ")
    print()
    
    if not all_articles:
        print("âš ï¸  æœ¬æœˆæš‚æ— æ–‡ç« æ•°æ®")
        return
    
    # æ’åæ–‡ç« 
    print("æ­¥éª¤ 2: æ–‡ç« æ’å")
    top_articles = rank_articles(all_articles, top_n=50)
    print(f"Top 50 æ–‡ç« å·²é€‰å‡º")
    print()
    
    # ç”ŸæˆæœˆæŠ¥æ–‡ä»¶
    print("æ­¥éª¤ 3: ç”ŸæˆæœˆæŠ¥æ–‡ä»¶")
    monthly_content = generate_monthly_content(start_date, end_date, top_articles)
    
    # ä¿å­˜æœˆæŠ¥æ–‡ä»¶
    monthly_dir = Path("monthly")
    monthly_dir.mkdir(exist_ok=True)
    
    # ä½¿ç”¨å¹´æœˆä½œä¸ºæ–‡ä»¶å
    month_str = datetime.strptime(start_date, "%Y-%m-%d").strftime("%Y-%m")
    monthly_file = monthly_dir / f"month-{month_str}.md"
    monthly_file.write_text(monthly_content, encoding="utf-8")
    print(f"æœˆæŠ¥æ–‡ä»¶å·²ä¿å­˜: {monthly_file}")
    print()
    
    # ç”ŸæˆREADMEæ‘˜è¦ï¼ˆTop 10ï¼‰
    print("æ­¥éª¤ 4: æ›´æ–° README")
    beijing_tz = timezone(timedelta(hours=8))
    beijing_time = datetime.now(beijing_tz)
    timestamp = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
    
    summary_lines = [f"> ğŸ• æœ€åæ›´æ–°ï¼š{timestamp} (åŒ—äº¬æ—¶é—´)", ""]
    summary_lines.append(f"**{year_month}ç²¾é€‰**")
    summary_lines.append("")
    
    for i, article in enumerate(top_articles[:10], 1):
        summary_lines.append(f"{i}. [{article['title']}]({article['url']}) - {article['score']} points, {article['comments']} comments")
    
    summary_lines.append("")
    summary_lines.append(f"ğŸ“ **[æŸ¥çœ‹å®Œæ•´æœˆæŠ¥](monthly/month-{month_str}.md)** | Top 50 çƒ­é—¨æ–‡ç« ")
    
    update_readme_monthly_section("\n".join(summary_lines))
    print("README å·²æ›´æ–°")
    print()
    
    print("=" * 50)
    print("âœ“ æœˆæŠ¥ç”Ÿæˆå®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    generate_monthly_summary()
